<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IFT 3150 - Amine Obeid</title>
</head>
<body>
    <header>
        <h1>IFT 3150 - Amine Obeid</h1>
        <h2>Titre : Réseaux Résiduels à Résolution Adaptative</h2>
        <h2>Superviseur : <a href="https://neo-x.github.io/">Glen Berseth</a> </h2>
        <h2>Date de début : 1 mai 2024</h2>
        <h2>Date de fin : TBD</h2>
    </header>

    <main>
        <h2>Description du projet: </h2>

        Je travaillerais principalement avec le professeur <a href="https://neo-x.github.io/">Glen Berseth</a> et son étudiante de doctorat <a href="https://leademeule.com/">Léa Demeule</a> <a href="https://openreview.net/forum?id=9hcsB4oYxG">réseaux résiduels à résolution adaptative (ARRNs).</a>. <br> <br>
        <b>En apprentissage profond, les réseaux à résolution fixe sont communément utilisés en raison de leur simplicité.</b> Cependant, ces réseaux ne prennent pas en compte la variété des résolutions <b>des données réelles; ils se conforment à une seule et unique résolution.</b> <br> <br>
        Un autre type de réseaux dits adaptatifs existe; <b>ils peuvent opérer à diverses résolutions, mais sont habituellement complexes</b> à implémenter et donc <b>peu couramment utilisés.</b> <br> <br>
        Les ARRNs sont la première classe de réseaux adaptatifs dont la facilité d'implémentation correspond à celle des réseaux à résolution fixe.<b>Ils s'intègrent bien aux techniques d'apprentissage profond existantes, et pourraient s'avérer bien utiles dans une variété de tâches.</b>         
        
        <h2>Progrès hebdomadaire: </h2>
        <ul> 
            <li>Semaine 13-14 : Ces deux semaines sont consacrés à l'écriture du rapport et la préparation de la présentation.</li>
            
            <li>Semaine 11-12 : Dans ces deux semaines, j'avais principalement accompli la re-écriture du code pour tracer les graphes afin d'avoir un code généralisable et réutilisable. J'avais aussi inspecter le papier de recherche qui introduit MULLER, un redimensionneur, pour obtenir quelques idées clés. En addition, j'avais appris davantage sur le FID (Fréchet Inception Distance), une métrique pour évaluer les modèles qui font de la diffusion, ce qu'on compte faire dans le futur du projet. Après avoir compris la théorie de cette métrique, j'ai implémenté une première version qui la calcule dans le code. </li>
                
            <li>Semaine 9-10 : Télécharger le jeu de données STL10 qui n’est pas déjà fourni par DRAC, donc pour ce faire, j’ai dû le transférer de mon ordinateur en utilisant le logiciel Globus. Après l’avoir sur mon compte Narval, j'ai aussi exécuté les trois types d’expériences sur ce jeu de données pour les comparer avec le modèle avant les nouvelles modifications et mis les résultats sur wandb. En se faisant, j’ai trouvé un petit bug dans le code et je l'ai corrigé. J'ai aussi appris davantage sur les pyramides Laplacienne en lisant le papier de recherche initial qui les a introduit. En plus, le code qui trace les graphes avait plusieurs inconvenients. Pour cette raison, Léa et moi avons re-écris ce code.</li>

            <li>Semaines 7-8 : Commencer à se rencontrer avec Léa deux fois par semaine, ces deux semaines ont été consacrées à ré-exécuter les expériences après que le code a subi quelques modifications pour tester les nouvelles performances. J’avais exécuté les trois types d’expériences suivants : sur des modèles baselines qu’on utilise pour comparer notre modèle, sur le modèle adaptatif qui est notre modèle, et sur le modèle adaptatif dans lequel on utilise la méthode de Fourier comme échantillonneur. Les expériences ont été exécutées sur trois jeux de données classiques utilisés pour entraîner et tester les modèles, CIFAR10, CIFAR100 et TinyImageNet. Toutes les expériences ont été mises sur wandb pour pouvoir mieux visualiser et comparer avec les performances précédentes. Pour ce faire, j'ai dû écrire un script python pour transformer les fichiers JSON qui ont été utilisés pour passer les arguments du code afin de s'adapter aux changements des arguments avec les nouvelles modifications.</li>

            <li>Semaines 5-6 : Apprendre davantage sur les optimiseurs tel que Adam et ce que ça fait dans le monde de l’apprentissage profond. Ainsi que comprendre plus le code en regardant en détail les arguments qu’on peut spécifier au code (tel que le nom du jeu de données qu’on passe, la taille des batchs, l’optimiseur utilisé et une plusieurs autres arguments) et apprendre ce que chacun fait. En plus, créer un compte sur wandb (weights and biases) et comprendre comment l’utiliser pour mieux visualiser les résultats des expériences effectuées. Finalement, j’avais téléchargé les jeux de données qu’on va utiliser (CIFAR10, CIFAR100 et Tiny Image Net) sur mon compte Narval pour pouvoir les utiliser. </li>

            <li>Semaine 3-4 : Configurations SSH nécessaires pour pouvoir se connecter aux clusters de "Compute Canada". Notamment, j’ai appris sur les clés SSH privées et publiques, j’ai généré une paire de clés pour moi et j’ai dû ajouter la partie publique à mon compte DRAC pour pouvoir me connecter aux clusters. Aussi, j’avais dû mettre la partie publique de ma clé sur mon compte github afin de connecter mon compte github à Narval. En outre, j’ai appris les bonnes normes d’utilisations des commandes git pour pouvoir collaborer dans le projet. Finalement, j’avais  commencé à regarder le code pré-existant et rentrer un peu plus en détail dans la théorie.</li>
            
            <li>Semaine 1-2 : Lecture du papier de recherche concernant le projet et discussion des éléments théoriques, création de ce site web et création d'un compte sur "Digital Research Alliance of Canada"(DRAC) pour pouvoir utiliser les clusters de Narval et Béluga et apprendre comment bien utiliser les clusters en faisant la différence entre un noeud de connexion et un noeud interactif, savoir comment demander un noeud interactif, et comment soumettre et interrompre un code aux GPUs. En plus, je me suis familiarisé avec l’IDE Vscode en détails pour le développement en Python, et j’ai installé les extensions nécessaires, puis j’ai appris comment bien utiliser les outils de débogage.</li>
        </ul>
        <h2>Environnement et contraintes techniques: </h2>
        Déploiement du logiciel pour entrainement et inférence sur les grappes de calcul de DRAC et possiblement d'autres institutions. Utilisation de conteneurs et d'environments virtuels Python. <br> 
        <h2>Architecture logicielle et modules principaux de travail: </h2>
        Logiciel bâti sur des modules PyTorch implémentant les composantes de l'ARRN. À partir de ce logiciel, des nouvelles expérimentations et variantes du réseau pourront être crées.
        <h2> Résumé du rapport : </h2>
        J'ai travaillé principalement avec le professeur Glen Berseth et son étudiante de doctorat Léa Demeule sur le projet de réseaux résiduels à résolution adaptative (ARRNs).
        <br>
        En apprentissage profond, notamment en vision d'ordinateur, les réseaux à résolution fixe sont communément utilisés en raison de la simplicité de leur implémentation. Cependant, ces réseaux sont limité à une unique résolution, et donc ne prennent pas en compte la variété des résolutions des données réelles. Un autre type de réseaux est dits adaptatifs, ils peuvent opérer à diverses résolutions, mais sont habituellement complexes à implémenter et donc peu couramment utilisés. Les ARRNs sont la première classe de réseaux adaptatifs dont la facilité d'implémentation correspond à celle des réseaux à résolution fixe, ce qui fait de ces réseaux généralisables et facile à implémenter et donc addresse les inconvénients des deux type de réseaux existants.
        <br>
        Dans ce rapport, je présenterais les notions que j'ai appris durant la session. Premièrement, je discuterais de la préparation de l'environnement afin de pouvoir travailler sur le code existant et contribuer au projet. Puis, je décrirais les notions théoriques d'apprentissage profond acquises. Finalement, je parlerais des contributions pratiques que j'ai faite au projet.

        <h2> Summary of the report : </h2>
        I have been working during this semester on helping Professor Glen Berseth and his PhD student Léa Demeule on the adaptive resolution residual networks (ARRNs) research project.
        <br>
        In deep learning, specifically in computer vision, fixed resolution networks are usually adopted due to the simplicity of their implementation. However, these networks are restricted to a single resolution, which fails to account for the variety of resolutions in real world data. Another type of networks is called adaptive, these networks can operate on different resolutions but are usually difficult and complex to implement and therefore avoided. ARRNs are the first class of adaptive networks that also share the ease of implementation of fixed resolution networks, which makes them both generalizable and easy to use and therefore address the shortcomings of both existing networks. 
        <br>
        In this report, I will be presenting the aspects I have learned during this semester. First, I will discuss how I set up the work environment to be able to work with the codebase and contribute to the project. Then, I will describe the theoretical notions of deep learning I have acquired. Finally, I will talk about my practical contributions that I have made to the project.

    </main>

</body>
</html>
